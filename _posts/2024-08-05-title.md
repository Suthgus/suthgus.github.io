---
layout: post
title:  "회귀 알고리즘과 모델 규제"
date : 2024-08-05 00:25:20 +0700
---
# 3-1 k-최근접 이웃 회귀

### k-최근접 이웃 회귀

지도 학습 알고리즘은 분류와 회귀로 나뉜다. 회귀는 임의의 어떤 숫자를 예측하는 것을 말한다.     
농어의 길이, 높이, 두께 데이터를 이용해서 무게를 예측할 수 있을까?    

k-최근접 이웃 알고리즘은 회귀에도 작동한다. 여기서 타깃은 어떤 클래스가 아니라, 임의의 수치이다.      
수치 예측 방법: 예측하려는 샘플에 가장 가까운 샘플 k개를 선택한다. 이웃의 샘플들의 수치의 평균을 구한다.      

농어의 길이로 무게를 예측한다면, 길이가 특성이고 무게가 타깃이 된다.      
![image](https://github.com/user-attachments/assets/209e1f06-eaf0-4d7f-bf46-7b2c195510e2)     
머신러닝 모델에 사용하기 전에 훈련세트와 테스트세트로 나눈다.     
그리고 사이킷런에 사용할 훈련세트는 2차원 배열이어야하기 때문에 넘파이의 reshape() 메서드를 통해 2차원 배열로 만들어주어야한다.      
![image](https://github.com/user-attachments/assets/0bb8bf57-54a6-4430-b2e1-19893751cf3f)     
![image](https://github.com/user-attachments/assets/a090a8b6-90b6-4688-8b8f-564b9b68a71f)     
- 넘파이는 배열의 크기를 자동으로 지정하는 기능을 제공하는데, 크기에 -1을 지정하면 나머지 원소 개수로 모두 채울 수 있다. 따라서 배열의 전체 원소 개수를 몰라도 사용할 수 있어 편리하다.      

### 결정계수 (R^2)     
![image](https://github.com/user-attachments/assets/70e794cd-2411-4918-885a-0fff34ceef72)     
![image](https://github.com/user-attachments/assets/d8675c5c-d054-4b0c-8fba-66054805f4cf)     
k-최근접 이웃 회귀 알고리즘 클래스는 KNeighborsRegressor이다.      
객체를 생성하고 fit() 메서드로 회귀 모델을 훈련했다.      
그렇게 나온 score은 회귀를 평가하는 **결정계수(R^2)**이다.      
(훈련세트를 사용해서 모델을 훈련하고 테스트세트로 모델을 평가했다.)     
![image](https://github.com/user-attachments/assets/326e54e1-94dd-4260-ba23-8e2719919a78)     
타깃이 예측에 가까워지면 (분모가 0에 가까워지기 때문에) 1에 가까운 값이 된다.      

앞에서 훈련한 모델을 사용해서 훈련세트의 R^2점수를 확인해보자.     
![image](https://github.com/user-attachments/assets/492e1741-32df-48f5-8406-f37f65c35e63)     
이상한 점을 찾았는가?      
모델을 훈련세트에 훈련하면 훈련세트에 잘 맞는 모델이 만들어지기 때문에 훈련세트의 점수가 좀 더 높게 나온다.      
- 과대적합: 훈련세트에서 높은 점수, 테스트세트에서 매우 낮은 점수 ; 새로운 샘플을 예측할 때 잘 동작하지 않을 것     
- 과소적합: 훈련세트 점수 < 테스트세트 점수 or 두 점수 모두 낮은 점수 ; 모델이 너무 단순해서 훈련세트에 적절히 훈련되지 않은 경우     
따라서 우리는 이 모델이 과소적합인 것을 발견했다. 모델을 복잡하게, 즉 훈련세트에 더 잘 맞게 만들어서 이 문제를 해결해보자.      
- 이웃의 개수(k)를 줄이기- 훈련세트의 국지적인 패턴에 민감해질 것임 // 반대로, 과대적합일 경우 k값을 늘리면 됨     
![image](https://github.com/user-attachments/assets/803c6076-6f5a-4635-a6ce-bde087dd36cb)     


# 3-2 선형회귀     
knn 모델은 한계가 있다.      
knn모델은 가장 가까운 이웃 샘플의 평균을 구해 수치를 예측한다.      
따라서 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측한다. 즉, 농어가 아무리 커도 무게가 더 늘어나지 않는다.     
따라서 다른 알고리즘을 사용해야한다.       

선형회귀: 특성과 타깃 사이의 관계를 가장 잘 나타내는 직선을 학습하는 알고리즘     
- y=ax+b 의 직선의 방정식에서 이 데이터에 가장 잘 맞는 a와 b를 찾는 것     
![image](https://github.com/user-attachments/assets/ea3177f7-1a60-4ae6-a128-c0a136f5bd7c)     
사이킷런의 모델 클래스들은 훈련(fit), 평가(score), 예측(predict)하는 메서드 이름이 모두 동일하다.     
- LinearRegression클래스가 찾은 a,b는 lr객체의 coef_와 intercept_속성에 저장되어있다.      
![image](https://github.com/user-attachments/assets/9caf8761-ec59-4fd9-a625-32cba910115c)     
- 모델 파라미터: coef_와 intercept와 같이 머신러닝 알고리즘이 찾은 (최적의) 값     
![image](https://github.com/user-attachments/assets/c3c3a4e6-5b87-4a40-a0c6-da025d81ecea)     
![image](https://github.com/user-attachments/assets/c77063ba-43e6-4148-a391-dd1d03ae9ed6)     
이렇게 훈련세트 범위를 벗어난 농어의 무게를 예측할 수 있고, 이 예측은 직선의 연장선에 있다.      
![image](https://github.com/user-attachments/assets/241b650d-c452-485b-9216-145cb5866707)     
결과가 전체적으로 높지 않은 것을 확인할 수 있고, 그래프 왼쪽 아래를 보면 무게가 음수가 될 수도 있다, 이는 현실에서 있을 수 없는 일이다.     

 ### 다항회귀      
최적의 곡선을 찾아보자. 이런 2차 방정식의 그래프를 그리려면 길이를 제곱한 항이 훈련세트에 추가되어야한다.      
이는 넘파이 column_stack()함수를 사용하여 농어 길이의 제곱 값을 원래 데이터 앞에 붙이면 된다.      
![image](https://github.com/user-attachments/assets/a5af639f-fe67-46a5-80b5-43d78084a18a)     
이렇게 제곱한 값을 원래의 배열 앞에 나란히 붙였다.      
이제 train_poly를 사용해 선형회귀모델을 다시 훈련해서 2차 방정식의 a,b,c를 찾아보자.      
![image](https://github.com/user-attachments/assets/508d08d9-5fb1-4765-8978-76ba97f6192a)     
![image](https://github.com/user-attachments/assets/7685e007-3f08-4bd3-a1c3-8fe55cf1a661)     
모델이 학습한 그래프는 ![image](https://github.com/user-attachments/assets/fabda719-e7c9-424d-90f1-296668ff7cf4) 이다.      
![image](https://github.com/user-attachments/assets/021ffcf4-0413-4b10-b6a8-f7371c3445ac)     
점수는 크게 높아졌지만, 여전히 테스트 세트의 점수가 여전히 높은 것으로 보아 과소적합이 아직 남아있는 것을 확인할 수 있다.      
조금 더 복잡한 모델이 필요하다.      

### 특성공학과 규제     
훈련세트보다 테스트세트의 점수가 높은 점(과소적합)을 해결하기 위해서는 더 고차항을 넣어야한다.       
우리는 길이말고도 높이와 두께 데이터가 있다.       
선형회귀는 특성이 많을수록 효과를 내기 때문에 높이와 두께를 다항회귀에 함께 적용해보자 - PolynomialFeatures 클래스 사용      
- 다중회귀: 여러 개의 특성을 사용하여 선형 회귀 모델을 훈련시키는 것      
- 특성공학: 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업 - 사이킷런의 도구 사용      
- 판다스: 데이터 분석 라이브러리, 인터넷에서 데이터를 바로 다운로드해 사용가능      
- 데이터 프레임: 판다스의 핵심 데이터 구조, 다차원 배열 다룰 수 있음, 넘파이 배열로 변환 가능      
특성 데이터 ![image](https://github.com/user-attachments/assets/9a169a48-fdbd-4204-927f-2b3539721fc8)      
타깃 데이터 ![image](https://github.com/user-attachments/assets/6be66fe2-9460-460d-abb8-c4a5bd84b9b9)      
이전과 동일하게 두 데이터를 훈련세트와 테스트세트로 나누어서 train_input, test_input, train_target, test_target을 만든다.      
이 데이터를 사용해 새로운 특성을 만든다.      

- 변환기: 사이킷런의 클래스 중에서 특성을 만들거나 전처리하기 위해 제공되는 클래스 - fit, transform      
- 우리가 사용할 변환기는 PolynomialFeatures 클래스이다.      
- 예시) ![image](https://github.com/user-attachments/assets/5b32070c-07a3-44cb-9a44-a3d7c63a9c71)      
![image](https://github.com/user-attachments/assets/5ebbef9a-77c9-4568-b1da-4c7fbce3790e)      
- 훈련(fit)을 해야 변환(transform)이 가능하다. - fit()메서드로 새롭게 만들 특성 조합을 찾고, transform()메서드로 데이터를 변환한다.      
- [2,3]이 [1.2.3.4.6.9.]로 바뀌었다. 기본적으로 각 특성을 제곱한 항을 추가하고, 특성끼리 서로 곱한 항을 추가한다.      
- 선형방정식의 절편은 항상 1인 특성과 곱해지는 계수이기 때문에, 선형모델은 자동으로 1을 추가하기 때문에 1이 포함되게 된다.      
- 1은 필요없으므로 include_bias=False로 지정해서 특성을 변환하면 된다.      
- ![image](https://github.com/user-attachments/assets/81415664-7f49-43e6-a4dd-6244b51522d9)      
- 특성의 제곱과 특성끼리 곱한 항만 추가되었다.      

  위의 방식으로 train_input을 변환시켜 train_poly에 저장해보고 배열크기를 확인해보자.      
  ![image](https://github.com/user-attachments/assets/3b407d1c-e1bb-499c-b69e-76715136fd40)      
9개의 특성이 어떻게 만들어졌는지 확인해보자      
![image](https://github.com/user-attachments/assets/21137e3e-9670-493c-9876-c10ec6836310)      
테스트세트를 변환하자 **훈련세트를 기준으로 테스트세트를 변환하는 습관**      
![image](https://github.com/user-attachments/assets/27db4eae-4fdf-4b05-b5be-62300b96500e)      
train_poly를 사용해 모델을 훈련해보자      
![image](https://github.com/user-attachments/assets/4a873b80-d956-498a-a6ac-9c80e9abad9e)      
![image](https://github.com/user-attachments/assets/e57cb4cd-fc76-4ba4-a0d1-490cf75a23a4)      
![image](https://github.com/user-attachments/assets/e53cd309-e513-4df3-aa79-f6bab87ad443)      
더이상 과소적합은 문제는 나타나지 않는다.      

![image](https://github.com/user-attachments/assets/640a91ec-001b-4923-b515-05967f962b57)      
위처럼 degree 매개변수를 이용해서 필요한 고차항의 최대 차수를 정할 수 있다.       
이 방법을 통해 특성의 개수를 늘리면 선형모델이 아주 강력해지면서 훈련세트에 대해 거의 완벽하게 학습해 과대 적합된다. 따라서 테스트 세트에서 형편없는 점수를 만든다.       
      
### 규제
- 규제: 머신러닝 모델이 훈련세트를 너무 과도하게 학습(과대적합)하지 못하도록 훼방하는 것      
- 선형 회귀 모델의 경우 특성에 곱해지는 계수(기울기)의 크기를 작게 만드는 일      
- 특성의 스케일이 정규화되지 않으면 곱해지는 계수 값도 차이나기 때문에 정규화를 먼저 해야한다.      
- ![image](https://github.com/user-attachments/assets/7e1099b0-4786-4e37-bf48-cef13790cc75)      
**훈련세트로 학습한 변환기를 사용해 테스트세트까지 변환해야한다**      

### 릿지 회귀
정규화한 데이터로 릿지 모델을 훈련해보자      
![image](https://github.com/user-attachments/assets/39232ab7-af52-4f68-862e-bfe5896d1d7a)      
![image](https://github.com/user-attachments/assets/c8460ec5-946b-4997-b5d5-a165846f382d)      
많은 특성을 사용했음에도 불구하고 훈련세트에 너무 과대적합 되지 않은 결과를 보여준다.       
- 규제의 양을 alpha 매개변수로 조절할 수 있다. (하이퍼파라미터: 사람이 지정)      
- 최적의 alpha 값을 찾아보자      
![image](https://github.com/user-attachments/assets/8298d925-9627-449e-b493-3725c15e6d2b)      
![image](https://github.com/user-attachments/assets/a252d0a3-e3d2-441a-9a8f-e87b897807df)      
![image](https://github.com/user-attachments/assets/8c101715-010e-4c3b-bbfc-0f5d076b8991)      
최적의 alpha 값은 두 그래프가 가장 가깝고 테스트 점수가 가장 높은 -1, 즉 0.1이다.      
alpha값을 0.1로 해서 최종모델을 훈련해보자.      
![image](https://github.com/user-attachments/assets/99e043ac-10d3-40b6-82fd-a774232db193)      
![image](https://github.com/user-attachments/assets/14c3a89d-3e31-4a4d-b8b8-8c4cda9f2300)      





